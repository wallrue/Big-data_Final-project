{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e797792",
   "metadata": {},
   "source": [
    "# IMPORT REQUIRED LIBRARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "400ac2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import codecs\n",
    "import gensim.corpora as corpora\n",
    "from snownlp import SnowNLP\n",
    "from snownlp import normal\n",
    "from snownlp import seg\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sentiment_modeling import Sentiment_classfifier\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Dropout, LSTM, Bidirectional\n",
    "from keras.models import Model, Input\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b929aae",
   "metadata": {},
   "source": [
    "# GET DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b978eb0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "path = os.path.abspath(os.getcwd()) \n",
    "filename = path+\"\\\\dataset\\\\organized_dataset\\\\csv-final.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f67b5917",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'encoding': 'UTF-16', 'confidence': 1.0, 'language': ''}\n"
     ]
    }
   ],
   "source": [
    "## Check data type\n",
    "import chardet\n",
    "with open(filename, 'rb') as file:\n",
    "    print(chardet.detect(file.read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f27f6817",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(filename, encoding = 'UTF-16', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8733cf61",
   "metadata": {},
   "source": [
    "### Cut the information of original email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5784731",
   "metadata": {},
   "outputs": [],
   "source": [
    "check = df['Body'].isnull()\n",
    "contents = [df['Body'][i] for i in range(len(check)) if check[i] == False] #Filter the email which has non content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c732dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(contents)):\n",
    "    for j in range(len(contents[i])):\n",
    "        if contents[i][j:j+len('SecuShare')] == 'SecuShare': #Cut the information for original email\n",
    "            contents[i] = contents[i][:j]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee568d24",
   "metadata": {},
   "source": [
    "### Preprocess function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d982c24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_chinese(mycontents):\n",
    "    #Filter to get a sentnece only chinese\n",
    "    for n in range(len(mycontents)):\n",
    "        temp_list = \"\"\n",
    "        for i in re.findall(\"[\\u4e00-\\u9fff]\",mycontents[n]):\n",
    "            temp_list += i\n",
    "        mycontents[n] = temp_list\n",
    "    return mycontents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da851956",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle(doc):\n",
    "    han_text = normal.zh2hans(doc) #Conver Traditional chinese to simplfied chinese\n",
    "    words = seg.seg(han_text) #segment the sentences\n",
    "    words = normal.filter_stop(words)\n",
    "    return words\n",
    "\n",
    "def sent_to_words(sentences): #Preprocess data\n",
    "    word_list = list()\n",
    "    for i in tqdm(range(len(sentences))):\n",
    "        word_list.append(handle(sentences[i]))\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e2f6cea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def preprocessing(data_words, max_length, token2id):\n",
    "    preprocessed_text = np.ones((len(data_words), max_length))*len(token2id) #Assign id=length for the word not in the id2word dictiornary \n",
    "    for row_id in tqdm(range(len(data_words))):\n",
    "        for col_id in range(max_length):\n",
    "            if col_id < len(data_words[row_id]):\n",
    "                if data_words[row_id][col_id] in token2id:\n",
    "                    preprocessed_text[row_id][col_id] = token2id[data_words[row_id][col_id]] #Assign id for the word in id2word dictiornary\n",
    "\n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8770c7a2",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e67f8b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = filter_chinese(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e113a42b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2525/2525 [05:48<00:00,  7.24it/s]\n"
     ]
    }
   ],
   "source": [
    "data_words = sent_to_words(contents)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d17ccb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = corpora.Dictionary(data_words)\n",
    "max_length = max([len(i) for i in data_words])\n",
    "token2id = id2word.token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a671ca34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 2525/2525 [00:00<00:00, 3435.61it/s]\n"
     ]
    }
   ],
   "source": [
    "preprocessed_text = preprocessing(data_words, max_length, token2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acdc540",
   "metadata": {},
   "source": [
    "# LABEL DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b511aa01",
   "metadata": {},
   "source": [
    "## Sentiment label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ba5b8c",
   "metadata": {},
   "source": [
    "#### Creating new label (if sentiment_label file is not existed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "743c9c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_modelpath = path+\"\\\\sentiment_modeling\\\\saved_model\\\\sentiment_bayes\\\\sentiment_bayes.marshal\"\n",
    "sentiment_classifier = Sentiment_classfifier(bayes_modelpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4ebd22d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2525/2525 [09:36<00:00,  4.38it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2525/2525 [00:36<00:00, 68.66it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 2525/2525 [00:00<00:00, 5529.59it/s]\n"
     ]
    }
   ],
   "source": [
    "sentiment_labels = sentiment_classifier.predict(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d979069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open file in write mode\n",
    "with open('sentiment_label.txt', 'w') as fp:\n",
    "    for item in sentiment_labels:\n",
    "        # write each item on a new line\n",
    "        fp.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709f9dc3",
   "metadata": {},
   "source": [
    "#### Reading available label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f287664",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_labels = []\n",
    "with open(r'sentiment_label.txt', 'r') as fp:\n",
    "    for line in fp:\n",
    "        x = line[:-1]\n",
    "        sentiment_labels.append(int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6c2cac39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to one hot vector:\n",
    "n_values = np.max(sentiment_labels) + 1\n",
    "sentiment_labels = np.eye(n_values)[sentiment_labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39a7edc",
   "metadata": {},
   "source": [
    "## Topic label (updating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ab4f566d",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_labels = np.random.randint(20,size=len(sentiment_labels)).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d32f1b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to one hot vector:\n",
    "n_values = np.max(topic_labels) + 1\n",
    "topic_labels = np.eye(n_values)[topic_labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52309bb4",
   "metadata": {},
   "source": [
    "# BUILDING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e137c8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLstm:\n",
    "    def __init__(self,vocabulary_size, seq_len):\n",
    "        \n",
    "        input_shape = Input((seq_len,))\n",
    "        embedding_layer = Embedding(vocabulary_size, 128, input_length=seq_len)(input_shape)\n",
    "        \n",
    "        bilstm_layer = Bidirectional(LSTM(64))(embedding_layer)\n",
    "        dropout_layer = Dropout(0.5)(bilstm_layer)\n",
    "        dense_layer = Dense(64, activation='relu')(dropout_layer)\n",
    "        \n",
    "        out1 = Dense(2, activation='softmax')(dense_layer)    \n",
    "        out2 = Dense(20, activation='softmax')(dense_layer)\n",
    "        \n",
    "        self.model = Model(input_shape, [out1,out2])\n",
    "        self.model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        \n",
    "        self.history\n",
    "\n",
    "    def fit(self, x, y1, y2,mybatch_size = 256,myepochs = 5):\n",
    "        self.history = self.model.fit(x, [y1,y2], batch_size=mybatch_size, epochs=myepochs)\n",
    "        \n",
    "    def prediction(self,x):\n",
    "        return self.model.predict(x)#.reshape(-1,1) ##size col = 41, row = 1 ==> y = (1,25949) //but only y[8],y[11] or y[12] keep the result\n",
    "           \n",
    "    def test(self,x,y):\n",
    "        y_pred = self.model.predict(x)\n",
    "        y_test = np.array(y)\n",
    "\n",
    "        print(\"Accuracy of testing: \",mylossfunction(y_pred,y_test))\n",
    "        #print(\"ROC Area: \")\n",
    "        #roc_error =  myROCfunction(y_test,y_pred)\n",
    "        #for i in range(len(roc_error)):\n",
    "        #  print(\"- Class \",i,\" = %0.2f\",roc_error[i])\n",
    "\n",
    "    def save_model(self,name):\n",
    "        self.model.save(name + \"-BiLSTM\")\n",
    "    \n",
    "    def load_model(self,name):\n",
    "        self.model = keras.models.load_model(name + \"-BiLSTM\")\n",
    "            \n",
    "    def history(self):\n",
    "        return self.history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2daa03c9",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f20440",
   "metadata": {},
   "source": [
    "### Define Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "87f506b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = preprocessed_text\n",
    "y1 = sentiment_labels\n",
    "y2 = topic_labels\n",
    "#y = np.hstack((topic_labels,sentiment_labels))\n",
    "# x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8529983a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = len(id2word) + 1\n",
    "seq_len = np.shape(X)[1] #Sequence length = sentence length\n",
    "input_dim = vocabulary_size \n",
    "input_length = seq_len\n",
    "mybatch_size = 256\n",
    "myepochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737cc4ae",
   "metadata": {},
   "source": [
    "### Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d37f2827",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "10/10 [==============================] - 794s 82s/step - loss: 3.5467 - dense_1_loss: 0.5471 - dense_2_loss: 2.9996 - dense_1_accuracy: 0.8432 - dense_2_accuracy: 0.0495\n",
      "Epoch 2/5\n",
      "10/10 [==============================] - 925s 93s/step - loss: 3.4038 - dense_1_loss: 0.3787 - dense_2_loss: 3.0250 - dense_1_accuracy: 0.8432 - dense_2_accuracy: 0.0562\n",
      "Epoch 3/5\n",
      "10/10 [==============================] - 1179s 120s/step - loss: 3.2585 - dense_1_loss: 0.2310 - dense_2_loss: 3.0275 - dense_1_accuracy: 0.8994 - dense_2_accuracy: 0.0586\n",
      "Epoch 4/5\n",
      "10/10 [==============================] - 1192s 120s/step - loss: 3.2008 - dense_1_loss: 0.1915 - dense_2_loss: 3.0093 - dense_1_accuracy: 0.9180 - dense_2_accuracy: 0.0634\n",
      "Epoch 5/5\n",
      "10/10 [==============================] - 1339s 136s/step - loss: 3.1573 - dense_1_loss: 0.1571 - dense_2_loss: 3.0002 - dense_1_accuracy: 0.9350 - dense_2_accuracy: 0.0578\n"
     ]
    }
   ],
   "source": [
    "myBiLSTM = BiLstm(input_dim, input_length)\n",
    "myBiLSTM.fit(X,y1,y2,mybatch_size,myepochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e540397",
   "metadata": {},
   "source": [
    "### Save model and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "476e9c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model\\customer_preferences_model-BiLSTM\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model\\customer_preferences_model-BiLSTM\\assets\n"
     ]
    }
   ],
   "source": [
    "myBiLSTM.save_model(\"saved_model\\\\customer_preferences_model\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b19f0056",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save parameters\n",
    "saved_dict = {'max_length': max_length, 'token2id': token2id}\n",
    "with codecs.open('saved_model\\\\token2id.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(saved_dict, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98471a4c",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0d3eedfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "myBiLSTM.load_model(\"saved_model\\\\customer_preferences_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5a473a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading parameters\n",
    "with codecs.open('saved_model\\\\token2id.json', 'r', 'utf-8') as data_file:\n",
    "    parameters = json.load(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5cb669eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.13it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1005.11it/s]\n"
     ]
    }
   ],
   "source": [
    "# Pre_processing by Preprocess function \n",
    "text = [contents[4]]\n",
    "text = filter_chinese(text)\n",
    "pre_data_words = sent_to_words(text)  \n",
    "data = preprocessing(pre_data_words, parameters['max_length'], parameters['token2id']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f0bb9991",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Prediction\n",
    "result = myBiLSTM.prediction(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "55711300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment [0.6516181  0.34838185]\n",
      "Topic [0.04384184 0.04987843 0.04468744 0.05048144 0.05874954 0.04817674\n",
      " 0.05743834 0.05562602 0.05087141 0.04837254 0.05310418 0.0419022\n",
      " 0.04885544 0.04682598 0.04890513 0.04816113 0.05538277 0.04923476\n",
      " 0.04305987 0.0564447 ]\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentiment\", result[0][0]) #List of probability of 'neg', 'pos'\n",
    "print(\"Topic\", result[1][0]) #Lis of probability of 20 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a23d9305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment [1. 0.]\n",
      "Topic [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Real result\n",
    "print(\"Sentiment\", y1[4])\n",
    "print(\"Topic\", y2[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f512a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
